import os
import faiss
import torch
import numpy as np
from fastapi import FastAPI, HTTPException
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader


# --- Configuration ---
EMBEDDING_MODEL_NAME = "models/esti-rag-ft"
DEFAULT_EMBEDDING_MODEL_NAME='sentence-transformers/all-MiniLM-L6-v2'
if os.path.exists(EMBEDDING_MODEL_NAME):
    print(f"üìÇ Chargement du mod√®le fine-tun√© depuis {EMBEDDING_MODEL_NAME}")
else:
    print(f"üåê Aucun mod√®le fine-tun√© trouv√©. Chargement du mod√®le de base : {DEFAULT_EMBEDDING_MODEL_NAME}")
    EMBEDDING_MODEL_NAME = DEFAULT_EMBEDDING_MODEL_NAME

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
TOP_K = 3
BATCH_SIZE = 4
EPOCHS = 2
WARMUP_STEPS = 10
model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)


def fine_tune_model(question, positive_docs, negative_docs):
    train_examples = []
    for doc in positive_docs:
        train_examples.append(InputExample(texts=[question, doc], label=1.0))
    for doc in negative_docs:
        train_examples.append(InputExample(texts=[question, doc], label=0.0))

    if not train_examples:
        print("‚ö†Ô∏è Aucun exemple pour fine-tuning.")
        return

    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)
    train_loss = losses.CosineSimilarityLoss(model)

    # üîç V√©rifier les param√®tres entra√Ænables
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"üß† Nombre de param√®tres entra√Ænables : {trainable_params}")
    if trainable_params == 0:
        print("‚ùå Aucun param√®tre √† entra√Æner. Abandon.")
        return

    print(f"üèãÔ∏è Fine-tuning sur {len(train_examples)} exemples...")
    model.train()  # Assure le mode training
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=EPOCHS,
        warmup_steps=WARMUP_STEPS,
        show_progress_bar=True
    )
    print("‚úÖ Fine-tuning termin√©.")

    # üíæ Sauvegarde temporaire et rechargement du mod√®le pour rafra√Æchir les poids
    model.save("models/esti-rag-ft")
    return SentenceTransformer("models/esti-rag-ft", device=DEVICE)

def evaluate_score_margin(model, question, positive_docs, negative_docs, device):
    model.eval()
    with torch.no_grad():
        q_emb = model.encode(question, convert_to_tensor=True, device=device)

        pos_scores = []
        for doc in positive_docs:
            d_emb = model.encode(doc, convert_to_tensor=True, device=device)
            pos_scores.append(torch.nn.functional.cosine_similarity(q_emb, d_emb, dim=0).item())

        neg_scores = []
        for doc in negative_docs:
            d_emb = model.encode(doc, convert_to_tensor=True, device=device)
            neg_scores.append(torch.nn.functional.cosine_similarity(q_emb, d_emb, dim=0).item())

    min_pos = min(pos_scores)
    max_neg = max(neg_scores)
    return min_pos, max_neg, pos_scores, neg_scores

def fine_tune_until_margin_respected(question, positive_docs, negative_docs,
                                     model, batch_size, epochs, warmup_steps, device,
                                     max_iterations=10):
    iteration = 0
    current_model = model

    while iteration < max_iterations:
        iteration += 1
        print(f"\nüîÑ It√©ration #{iteration} de fine-tuning...")

        train_examples = [
            InputExample(texts=[question, doc], label=1.0) for doc in positive_docs
        ] + [
            InputExample(texts=[question, doc], label=0.0) for doc in negative_docs
        ]

        if not train_examples:
            print("‚ö†Ô∏è Aucun exemple pour fine-tuning.")
            break

        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
        train_loss = losses.CosineSimilarityLoss(current_model)

        current_model.train()
        current_model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            warmup_steps=warmup_steps,
            show_progress_bar=True
        )
        print("‚úÖ Fine-tuning termin√© pour cette it√©ration.")

        min_pos, max_neg, pos_scores, neg_scores = evaluate_score_margin(
            current_model, question, positive_docs, negative_docs, device
        )

        print(f"üìà Scores positifs : {['%.4f' % s for s in pos_scores]}")
        print(f"üìâ Scores n√©gatifs : {['%.4f' % s for s in neg_scores]}")
        print(f"‚úÖ min(score_positif) = {min_pos:.4f}")
        print(f"‚ùå max(score_n√©gatif) = {max_neg:.4f}")

        if min_pos > max_neg:
            print("üéØ Condition atteinte : tous les positifs sont mieux scor√©s que tous les n√©gatifs.")
            break
        else:
            print("üîÅ Encore des n√©gatifs mieux scor√©s que des positifs. On continue...")

    current_model.save("models/esti-rag-ft")
    return SentenceTransformer("models/esti-rag-ft", device=device)

def fine_tune_with_multiple_negatives(question, positive_docs, model, batch_size, epochs, warmup_steps, device):
    # Cr√©er des paires question <-> positive
    train_examples = [InputExample(texts=[question, doc]) for doc in positive_docs]

    if not train_examples:
        print("‚ö†Ô∏è Aucun exemple pour fine-tuning.")
        return None

    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
    train_loss = losses.MultipleNegativesRankingLoss(model)

    print(f"üèãÔ∏è Fine-tuning avec MultipleNegativesRankingLoss sur {len(train_examples)} exemples...")

    model.train()
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=warmup_steps,
        show_progress_bar=True
    )
    print("‚úÖ Fine-tuning termin√©.")

    model.save("models/esti-rag-ft")
    return SentenceTransformer("models/esti-rag-ft", device=device)

